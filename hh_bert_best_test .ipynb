{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hh_bert_best_test.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Multi-label text classification, using BErT model"
      ],
      "metadata": {
        "id": "Jcwgxyvs9zMT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Contents:\n",
        "\n",
        "* Imports and data loading\n",
        "* Texts preprocessing\n",
        "* Bert training\n",
        "* Bert testing"
      ],
      "metadata": {
        "id": "cuPV1-9bBBgC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports and data loading"
      ],
      "metadata": {
        "id": "Lh3WObfwz5Qi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save model\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ucgOPVYp3F1",
        "outputId": "913a2890-837a-4e21-b13e-376a1aa2d2af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing the transformers library and additional libraries if looking process \n",
        "\n",
        "!pip install -q transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGz2N6OunIwq",
        "outputId": "231d64db-0842-4a0f-ff06-30ee1c0bfbf2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 3.8 MB 2.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 61.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 43.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 46.1 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install razdel\n",
        "!pip install pymorphy2\n",
        "!pip install slovnet\n",
        "!pip install navec"
      ],
      "metadata": {
        "id": "OhtMo7LniZPi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.metrics as skm\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "import transformers\n",
        "from transformers import BertTokenizer, BertModel, BertConfig\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from transformers import BertForSequenceClassification"
      ],
      "metadata": {
        "id": "P7jSoII1u74F"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "from navec import Navec\n",
        "from slovnet import NER\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import string\n",
        "import random\n",
        "from razdel import tokenize \n",
        "import pymorphy2\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt \n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "nltk_stop_words = stopwords.words('russian') + list(string.punctuation)\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "!wget https://storage.yandexcloud.net/natasha-navec/packs/navec_news_v1_1B_250K_300d_100q.tar\n",
        "!wget https://storage.yandexcloud.net/natasha-slovnet/packs/slovnet_ner_news_v1.tar\n",
        "\n",
        "navec = Navec.load('navec_news_v1_1B_250K_300d_100q.tar')\n",
        "ner = NER.load('slovnet_ner_news_v1.tar')\n",
        "ner.navec(navec)"
      ],
      "metadata": {
        "id": "qQ1ROLNTP7w8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train-test datasets\n",
        "\n",
        "!wget https://boosters.pro/api/ch/files/pub/HeadHunter_train.csv\n",
        "!wget https://boosters.pro/api/ch/files/pub/HeadHunter_test.csv"
      ],
      "metadata": {
        "id": "8HkXvYEqPVtV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9f2c149-44da-4af6-ffcd-862dc86b19d1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-07 20:21:37--  https://boosters.pro/api/ch/files/pub/HeadHunter_train.csv\n",
            "Resolving boosters.pro (boosters.pro)... 91.206.14.169\n",
            "Connecting to boosters.pro (boosters.pro)|91.206.14.169|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 24692086 (24M) [application/octet-stream]\n",
            "Saving to: ‘HeadHunter_train.csv’\n",
            "\n",
            "HeadHunter_train.cs 100%[===================>]  23.55M  13.3MB/s    in 1.8s    \n",
            "\n",
            "2022-03-07 20:21:39 (13.3 MB/s) - ‘HeadHunter_train.csv’ saved [24692086/24692086]\n",
            "\n",
            "--2022-03-07 20:21:40--  https://boosters.pro/api/ch/files/pub/HeadHunter_test.csv\n",
            "Resolving boosters.pro (boosters.pro)... 91.206.14.169\n",
            "Connecting to boosters.pro (boosters.pro)|91.206.14.169|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10723842 (10M) [application/octet-stream]\n",
            "Saving to: ‘HeadHunter_test.csv’\n",
            "\n",
            "HeadHunter_test.csv 100%[===================>]  10.23M  9.51MB/s    in 1.1s    \n",
            "\n",
            "2022-03-07 20:21:41 (9.51 MB/s) - ‘HeadHunter_test.csv’ saved [10723842/10723842]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.read_csv('HeadHunter_test.csv')\n",
        "df_test.fillna('неизвестно', inplace=True)\n",
        "\n",
        "df_train = pd.read_csv('HeadHunter_train.csv')\n",
        "df_test.fillna('неизвестно', inplace=True)"
      ],
      "metadata": {
        "id": "D0vKl3NDhdH4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class imbalance\n",
        "df_train.target.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TXe5bVpCiDh",
        "outputId": "b85b99c7-28a6-4f08-d858-c72d52b4be90"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8          24093\n",
              "0          21003\n",
              "1,8         1476\n",
              "1           1269\n",
              "3            905\n",
              "6,8          473\n",
              "6            368\n",
              "7            326\n",
              "3,8          209\n",
              "1,6          141\n",
              "5,8          121\n",
              "5            102\n",
              "1,5           78\n",
              "1,6,8         48\n",
              "4             38\n",
              "4,8           36\n",
              "1,5,8         33\n",
              "7,8           25\n",
              "1,4           24\n",
              "1,7           15\n",
              "5,7           13\n",
              "2             12\n",
              "1,3            9\n",
              "5,6            9\n",
              "1,5,6          7\n",
              "5,6,8          6\n",
              "4,6            5\n",
              "3,7            5\n",
              "1,4,8          4\n",
              "1,4,6          3\n",
              "6,7            3\n",
              "1,3,8          3\n",
              "1,3,5          2\n",
              "1,7,8          2\n",
              "1,5,6,8        1\n",
              "3,6            1\n",
              "5,7,8          1\n",
              "4,6,8          1\n",
              "1,3,6          1\n",
              "3,6,8          1\n",
              "3,5            1\n",
              "1,2,6          1\n",
              "3,5,8          1\n",
              "3,5,7          1\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Texts preprocessing"
      ],
      "metadata": {
        "id": "juUmRk06z9_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def list_to_ints(x):\n",
        "  l = []\n",
        "  for stroka in x.split(','):\n",
        "    l.append(int(stroka.split()[0]))\n",
        "  return l\n",
        "\n",
        "def int_list(x):\n",
        "  l = []\n",
        "  l.append(x)\n",
        "  return l\n",
        "\n",
        "# one-hot-encoding\n",
        "def to_ohe(idx, num_of_classes=9):\n",
        "  arr = [0]*num_of_classes\n",
        "  for i in idx:\n",
        "    arr[i] = 1\n",
        "  return arr "
      ],
      "metadata": {
        "id": "Izp46pyB0abu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate synthetic data (reviews)\n",
        "def generate_reviews(dataframe, text_column, target_column, label, k, words_in_review):\n",
        "  list_of_tokens = []\n",
        "  for row in dataframe[dataframe[target_column] == label][text_column].values:\n",
        "    tokens = list(tokenize(row))\n",
        "    list_of_words = [t.text for t in tokens]\n",
        "    list_of_tokens.extend(list_of_words)\n",
        "  list_of_lemmas = [morph.parse(x)[0].word for x in list_of_tokens]\n",
        "  counter_of_lemmas = Counter(list_of_lemmas)\n",
        "  list_of_words = [random.choices(population=list(counter_of_lemmas.keys()), \\\n",
        "                                  weights=counter_of_lemmas.values(), k=words_in_review) for _ in range(k)]\n",
        "  reviews = []\n",
        "  for rev in list_of_words:\n",
        "    reviews.append(' '.join([x for x in rev]))\n",
        "  return reviews"
      ],
      "metadata": {
        "id": "JqFFKUoj0bvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate specific number of reviews to balance classes \n",
        "def paraphraser(df_fe, df_orig):\n",
        "  pos_words_counter = {}\n",
        "  for x in range(1,8):\n",
        "    pos_words_counter[x] = np.sum(df_orig[df_orig.target == str(x)].positive.str.len()) / len(df_orig[df_orig.target == str(x)])\n",
        "\n",
        "  neg_words_counter = {}\n",
        "  for x in range(1,8):\n",
        "    neg_words_counter[x] = np.sum(df_orig[df_orig.target == str(x)].negative.str.len()) / len(df_orig[df_orig.target == str(x)])\n",
        "\n",
        "  list_reviews_all_labels_pos = {}\n",
        "  for k,v in tqdm(pos_words_counter.items()):\n",
        "    list_reviews_all_labels_pos[k] = generate_reviews(df_orig, 'positive', 'target', '1', 5000, int(v))\n",
        "\n",
        "  list_reviews_all_labels_neg = {}\n",
        "\n",
        "  for k,v in tqdm(neg_words_counter.items()):\n",
        "    list_reviews_all_labels_neg[k] = generate_reviews(df_orig, 'negative', 'target', '1', 5000, int(v))\n",
        "\n",
        "  labels_count = [3117,   13, 1139,  111,  376, 1069,  391]\n",
        "  df_sint_pos = pd.DataFrame(columns = ['reviews', 'target']) \n",
        "  for x in list_reviews_all_labels_pos.keys(): \n",
        "    l1 = list_reviews_all_labels_neg[x]\n",
        "    l2 = [x]*5000\n",
        "    df_interim = pd.DataFrame({'reviews': l1, 'target':l2})\n",
        "    df_interim = df_interim.sample(n=5000-labels_count[x-1]) \n",
        "    df_sint_pos =  pd.concat([df_sint_pos, df_interim])\n",
        "\n",
        "  df_sint_pos.target.value_counts()\n",
        "\n",
        "  df_sint_neg = pd.DataFrame(columns = ['reviews2', 'target']) \n",
        "  for x in list_reviews_all_labels_pos.keys(): \n",
        "    l1 = list_reviews_all_labels_neg[x]\n",
        "    l2 = [x]*5000\n",
        "    df_interim = pd.DataFrame({'reviews2': l1, 'target':l2})\n",
        "    df_interim = df_interim.sample(n=5000-labels_count[x-1]) \n",
        "    df_sint_neg =  pd.concat([df_sint_neg, df_interim])\n",
        "  print(len(df_sint_neg))\n",
        "\n",
        "  df_sint_neg = df_sint_neg.sort_values('target').reset_index().drop('index', axis=1)\n",
        "  df_sint_pos = df_sint_pos.sort_values('target').reset_index().drop('index', axis=1)\n",
        "  df_sint = pd.merge(df_sint_pos, df_sint_neg, left_index=True, right_index=True)\n",
        "\n",
        "  df_sint['reviews'] = df_sint['reviews'] + df_sint['reviews2']\n",
        "  df_sint.drop(['target_y', 'reviews2'], axis=1, inplace=True)\n",
        "\n",
        "  df_sint.rename(columns = {'reviews':'review', 'target_x':'target'}, inplace=True)\n",
        "  df_sint = df_sint[['target', 'review', ]]\n",
        "\n",
        "  df_sint.target = df_sint.target.apply(int_list)\n",
        "\n",
        "  df_fe = pd.concat([df_fe, df_sint], ignore_index=True)\n",
        "\n",
        "  mask0 = df_fe.target.isin([[0]])\n",
        "  mask8 = df_fe.target.isin([[8]])\n",
        "  df0 = df_fe[mask0].sample(n=5000)\n",
        "  df8 = df_fe[mask8].sample(n=5000)\n",
        "  df_fe = df_fe[~mask0]\n",
        "  df_fe = df_fe[~mask8]\n",
        "\n",
        "  df_fe = pd.concat([df_fe, df0, df8])\n",
        "  \n",
        "  return df_fe\n"
      ],
      "metadata": {
        "id": "8HGDkCynBw8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Feature engineering and applying paraphraser function\n",
        "def feature_engineering(df_fe, df_orig, type_of_df='train'):\n",
        "  \n",
        "  df_fe.drop('review_id', axis=1, inplace=True)\n",
        "  df_fe[['salary_rating', 'team_rating', 'managment_rating', 'career_rating',\n",
        "       'workplace_rating', 'rest_recovery_rating']] = df_fe[['salary_rating', 'team_rating', 'managment_rating', 'career_rating',\n",
        "       'workplace_rating', 'rest_recovery_rating']].replace({1:\"один\",\n",
        "                                                             2:\"два\",\n",
        "                                                             3:\"три\",\n",
        "                                                             4:\"четыре\",\n",
        "                                                             5:\"пять\"})\n",
        "  cols_eng = ['city', 'position', 'positive', 'negative', 'salary_rating',\n",
        "       'team_rating', 'managment_rating', 'career_rating', 'workplace_rating',\n",
        "       'rest_recovery_rating']\n",
        "\n",
        "  cols_rus = ['город', \"позиция\", \"негативный\", \"позитивный\", \"рейтинг зарплаты\", \n",
        "                        \"рейтинг команды\", \"рейтинг менеджмента\", \"рейтинг карьеры\", \n",
        "                        \"рейтинг рабочего места\", \"рейтинг места отдыха\"]\n",
        "\n",
        "  df_fe.rename(columns=dict(zip(cols_eng, cols_rus)), inplace=True)\n",
        "  cols_to_val = [ \"рейтинг зарплаты\", \n",
        "                       \"рейтинг команды\", \"рейтинг менеджмента\", \"рейтинг карьеры\", \n",
        "                       \"рейтинг рабочего места\", \"рейтинг места отдыха\"]\n",
        "  for col in cols_to_val:  \n",
        "    df_fe[col] = df_fe[col].apply(lambda x: str(x) + ' ' + str(col))\n",
        "  \n",
        "  df_fe['review'] = ' '\n",
        "  for col in cols_rus:\n",
        "    df_fe['review'] = df_fe['review'] + df_fe[col] + str(' ') \n",
        "  df_fe.drop([x for x in cols_rus], axis=1, inplace=True)\n",
        "  \n",
        "  if type_of_df == 'train':\n",
        "    df_fe.target = df_fe.target.apply(list_to_ints)\n",
        "    #import paraphraser function\n",
        "    df_fe = paraphraser(df_fe, df_orig)\n",
        "\n",
        "    df_fe.target = df_fe.target.apply(to_ohe)\n",
        "  elif type_of_df == 'test':\n",
        "    pass\n",
        "  else:\n",
        "    print('Choose type of df')\n",
        "    \n",
        "  return df_fe"
      ],
      "metadata": {
        "id": "cbRn7bO30Hos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# making two datasets\n",
        "df_orig = df_train.copy()\n",
        "df_fe = df_train.copy()\n",
        "\n",
        "#main function\n",
        "df_bert = feature_engineering(df_fe, df_orig, type_of_df='train')"
      ],
      "metadata": {
        "id": "Z-s3OdoXZyyW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04fc9ec5-286d-43b5-9e92-f6bc0fd4c48c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7/7 [01:00<00:00,  8.57s/it]\n",
            "100%|██████████| 7/7 [01:37<00:00, 13.91s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28784\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bert training"
      ],
      "metadata": {
        "id": "hutqkvhp1i56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataframe, tokenizer, max_len, regime='train'):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.review = dataframe.review\n",
        "        self.max_len = max_len\n",
        "        self.regime = regime\n",
        "        if self.regime == 'train':\n",
        "          self.targets = self.data.target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.review)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        review = str(self.review[index])\n",
        "        review = \" \".join(review.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            review,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=False\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "\n",
        "        if self.regime == 'train':\n",
        "          return {\n",
        "              'ids': torch.tensor(ids, dtype=torch.long),\n",
        "              'mask': torch.tensor(mask, dtype=torch.long),\n",
        "              'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
        "          }\n",
        "        elif self.regime == 'test':\n",
        "          return {\n",
        "              'ids': torch.tensor(ids, dtype=torch.long),\n",
        "              'mask': torch.tensor(mask, dtype=torch.long),\n",
        "              #'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
        "          }\n",
        "\n"
      ],
      "metadata": {
        "id": "gsJFmZdlAIVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = 'cointegrated/rubert-tiny2'\n",
        "model = BertForSequenceClassification.from_pretrained(model_path)\n",
        "\n",
        "class BertClassifier:\n",
        "\n",
        "    def __init__(self,dataframe, model_path, tokenizer_path, n_classes=2, epochs=1, save_path='/content/bert.pt', regime='train',\n",
        "                 train_batch_size=32, max_seq_len=512, prob_sigmoid_threshhold=0.2, learning_rate=2e-4):\n",
        "        self.model_save_path=save_path\n",
        "\n",
        "        if regime == 'train':\n",
        "          self.model = BertForSequenceClassification.from_pretrained(model_path)\n",
        "          self.out_features = self.model.bert.encoder.layer[1].output.dense.out_features\n",
        "          self.model.classifier = torch.nn.Linear(self.out_features, n_classes)\n",
        "          self.regime = 'train'\n",
        "          \n",
        "        elif regime == 'test':\n",
        "          self.model = torch.load(self.model_save_path, map_location ='cpu')\n",
        "          self.regime = 'test'\n",
        "\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.max_len = max_seq_len\n",
        "        self.epochs = epochs\n",
        "        self.model.to(self.device)  \n",
        "        self.df = dataframe\n",
        "        self.train_batch_size = train_batch_size\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.prob_sigmoid_threshhold = prob_sigmoid_threshhold\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "\n",
        "    \n",
        "    def preparation(self):\n",
        "\n",
        "        train_size = 0.8\n",
        "        train_dataset=self.df.sample(frac=train_size,random_state=42)\n",
        "        test_dataset=self.df.drop(train_dataset.index).reset_index(drop=True)\n",
        "        train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "\n",
        "        print(\"FULL Dataset: {}\".format(self.df.shape))\n",
        "        print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
        "        print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
        "\n",
        "        self.training_set = CustomDataset(train_dataset, self.tokenizer, self.max_len, regime=self.regime)\n",
        "        self.testing_set = CustomDataset(test_dataset, self.tokenizer, self.max_len, regime=self.regime)\n",
        "\n",
        "        # create data loaders\n",
        "        self.train_loader = DataLoader(self.training_set, batch_size=self.train_batch_size, shuffle=True)\n",
        "        self.valid_loader = DataLoader(self.testing_set, batch_size=8,  shuffle=True)\n",
        "\n",
        "        # helpers initialization\n",
        "        self.optimizer = AdamW(self.model.parameters(), lr=self.learning_rate, correct_bias=False)\n",
        "        self.scheduler = get_linear_schedule_with_warmup(\n",
        "                self.optimizer,\n",
        "                num_warmup_steps=0,\n",
        "                num_training_steps=len(self.train_loader) * self.epochs\n",
        "            )\n",
        "        self.loss_fn = torch.nn.BCEWithLogitsLoss().to(self.device)\n",
        "            \n",
        "    def fit(self):\n",
        "        self.model = self.model.train()\n",
        "        losses = []\n",
        "        correct_predictions = 0\n",
        "\n",
        "        for data in self.train_loader:\n",
        "            input_ids = data[\"ids\"].to(self.device)\n",
        "            attention_mask = data[\"mask\"].to(self.device)\n",
        "            targets = data[\"targets\"].to(self.device)\n",
        "\n",
        "            outputs = self.model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask\n",
        "                )\n",
        "            \n",
        "            loss = self.loss_fn(outputs.logits, targets)\n",
        "  \n",
        "            preds = torch.sigmoid(outputs.logits)  # torch.Size([N, C]) e.g. tensor([[0., 0.5, 0.]])\n",
        "            preds[preds >= self.prob_sigmoid_threshhold] = 1\n",
        "\n",
        "\n",
        "            summa = torch.sum(preds == targets)\n",
        "\n",
        "            correct_predictions += summa\n",
        "\n",
        "            losses.append(loss.item())\n",
        "            \n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "            self.optimizer.step()\n",
        "            self.scheduler.step()\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "        train_acc = correct_predictions.double() / len(self.training_set)\n",
        "        train_loss = np.mean(losses)\n",
        "        return train_acc, train_loss\n",
        "    \n",
        "    def eval(self):\n",
        "        self.model = self.model.eval()\n",
        "        losses = []\n",
        "        correct_predictions = 0\n",
        "        predslist = []\n",
        "        targetslist = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data in self.valid_loader:\n",
        "                input_ids = data[\"ids\"].to(self.device)\n",
        "                attention_mask = data[\"mask\"].to(self.device)\n",
        "                targets = data[\"targets\"].to(self.device)\n",
        "\n",
        "                outputs = self.model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask\n",
        "                    )\n",
        "\n",
        "                preds = torch.sigmoid(outputs.logits)  # torch.Size([N, C]) e.g. tensor([[0., 0.5, 0.]])\n",
        "                preds[preds >= self.prob_sigmoid_threshhold] = 1\n",
        "                \n",
        "                loss = self.loss_fn(outputs.logits, targets)\n",
        "\n",
        "                correct_predictions += torch.sum(preds == targets)\n",
        "                losses.append(loss.item())\n",
        "\n",
        "                #f1\n",
        "                preds[preds < self.prob_sigmoid_threshhold] = 0\n",
        "                predslist.append(preds.cpu())\n",
        "                targetslist.append(targets.cpu())\n",
        "        \n",
        "\n",
        "        predslist = np.concatenate(predslist)\n",
        "        targetslist = np.concatenate(targetslist)\n",
        "        f1_score_macro = skm.f1_score(targetslist, predslist, average='macro')\n",
        "        print(f1_score_macro)\n",
        "        cm = skm.multilabel_confusion_matrix(targetslist, predslist)\n",
        "        print(cm)\n",
        "        print(skm.classification_report(targetslist, predslist))\n",
        "\n",
        "        val_acc = correct_predictions.double() / len(self.testing_set)\n",
        "        val_loss = np.mean(losses)\n",
        "        return val_acc, val_loss, f1_score_macro\n",
        "    \n",
        "    def train(self):\n",
        "        best_f1 = 0\n",
        "        for epoch in range(self.epochs):\n",
        "            print(f'Epoch {epoch + 1}/{self.epochs}')\n",
        "            _, train_loss = self.fit()\n",
        "            print(f'Train loss {train_loss}')\n",
        "\n",
        "            _, val_loss, f1_score_eval = self.eval()\n",
        "            print(f'Val loss {val_loss}')\n",
        "            print('-' * 10)\n",
        "            if f1_score_eval > best_f1:\n",
        "                torch.save(self.model, self.model_save_path)\n",
        "                best_f1 = f1_score_eval\n",
        "        self.model = torch.load(self.model_save_path)\n",
        "    \n",
        "    def predict(self, text):\n",
        "\n",
        "      \n",
        "      inputs = self.tokenizer.encode_plus(\n",
        "          text,\n",
        "          None,\n",
        "          add_special_tokens=True,\n",
        "          max_length=self.max_len,\n",
        "          return_token_type_ids=False,\n",
        "          pad_to_max_length=True\n",
        "\n",
        "      )\n",
        "\n",
        "\n",
        "      out = {\n",
        "            'input_ids': torch.tensor(inputs['input_ids'],dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(inputs['attention_mask'],  dtype=torch.float)\n",
        "        }\n",
        "      \n",
        "      input_ids = out[\"input_ids\"].to(self.device)\n",
        "      attention_mask = out[\"attention_mask\"].to(self.device)\n",
        "\n",
        "      outputs = self.model(\n",
        "          input_ids=input_ids.unsqueeze(0),\n",
        "          attention_mask=attention_mask.unsqueeze(0)\n",
        "      )\n",
        "      \n",
        "      # print(outputs.logits)\n",
        "      preds = torch.sigmoid(outputs.logits)  # torch.Size([N, C]) e.g. tensor([[0., 0.5, 0.]])\n",
        "      print(f'after sigm {preds}')\n",
        "      preds[preds >= 0.4] = 1\n",
        "      list_of_preds = [i for i,j in enumerate(preds[0]) if j == 1]\n",
        "      if list_of_preds == []:\n",
        "        list_of_preds.append(torch.argmax(preds[0]).cpu().detach().numpy().tolist())\n",
        "\n",
        "      return list_of_preds "
      ],
      "metadata": {
        "id": "GMPP1TQvCwLG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a797235-c6b8-4bf6-eb13-4b20ca8716cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cointegrated/rubert-tiny2 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_name = 'hh_classifier.pt'\n",
        "save_path = f\"/content/gdrive/My Drive/{model_save_name}\"\n",
        "\n",
        "classifier = BertClassifier(\n",
        "        dataframe=df_bert,\n",
        "        model_path='cointegrated/rubert-tiny2',\n",
        "        tokenizer_path='cointegrated/rubert-tiny2',\n",
        "        n_classes=9,\n",
        "        epochs=8,\n",
        "        save_path=save_path,\n",
        "        regime='train',\n",
        "        train_batch_size=16, \n",
        "        max_seq_len=1024, \n",
        "        prob_sigmoid_threshhold=0.2, \n",
        "        learning_rate=3e-5\n",
        ")\n"
      ],
      "metadata": {
        "id": "le9FKfxCFXp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.preparation()\n",
        "classifier.fit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "ay3rponQ5gTe",
        "outputId": "525a8d87-58da-4e06-ed3f-4214012ece84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FULL Dataset: (44564, 2)\n",
            "TRAIN Dataset: (35651, 2)\n",
            "TEST Dataset: (8913, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-fe5215d58928>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreparation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-e7e1ec1e593a>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bert testing"
      ],
      "metadata": {
        "id": "rTwm8la95nFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = BertClassifier(\n",
        "        dataframe=df_bert,\n",
        "        model_path='cointegrated/rubert-tiny2',\n",
        "        tokenizer_path='cointegrated/rubert-tiny2',\n",
        "        n_classes=9,\n",
        "        epochs=8,\n",
        "        save_path=save_path,\n",
        "        regime='test',\n",
        "        train_batch_size=16, \n",
        "        max_seq_len=1024, \n",
        "        prob_sigmoid_threshhold=0.2, \n",
        "        learning_rate=3e-5\n",
        ")"
      ],
      "metadata": {
        "id": "vNJzH_h05q6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p = []\n",
        "for x in tqdm(range(len(df_bert))):\n",
        "  p.append(classifier.predict(df_bert.iloc[x].review))\n",
        "  print(x)"
      ],
      "metadata": {
        "id": "c8mJBtDJcAhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_res = pd.DataFrame({'review_id': df_test.review_id, 'target': p})"
      ],
      "metadata": {
        "id": "nBKLK5JOkz4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_res.target = df_res.target.apply(lambda x: ','.join(map(str, x)))"
      ],
      "metadata": {
        "id": "YdYn4nNsoPm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_res.target.value_counts()"
      ],
      "metadata": {
        "id": "p5cI8KtGqXp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# result to csv file\n",
        "df_res.to_csv('/content/result.csv', index=False)"
      ],
      "metadata": {
        "id": "Vw8s2exipCPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gEMHlmK9rcfS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}